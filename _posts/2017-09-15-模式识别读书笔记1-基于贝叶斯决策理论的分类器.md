---
title: 模式识别读书笔记1 基于贝叶斯决策理论的分类器
categories:
 - 模式识别与机器学习
tags:
 - 模式识别
 - 贝叶斯
description: 模式识别是根据对象特征值将其分类，下面介绍的方法以特征值的统计概率为基础。本文是《模式识别》第2章的笔记。
---

#  引言
&emsp;&emsp;模式识别是根据对象特征值将其分类，下面介绍的方法以特征值的统计概率为基础。本文是《模式识别》第2章的笔记。
##  为什么可用Bayes决策理论分类?

&emsp;&emsp;人们根据<big>**不确定性信息**</big>作出推理和决策需要对各种结论的概率作出估计，这类推理称为概率推理。贝叶斯推理的问题是条件概率推理问题，这一领域的探讨对揭示人们对概率信息的认知加工过程与规律、指导人们进行有效的学习和判断决策都具有十分重要的理论意义和实践意义。

### (1)样本的不确定性	

 &emsp;&emsp;1. 样本从总体中抽取，特征值都是<big>**随机变量**</big>，在相同条件下重复观测取值不同，<big>**故x为随机向量**</big>。  
&emsp;&emsp;2. 特征选择的不完善引起的不确定性。  
&emsp;&emsp;3. 测量中有随机噪声存在  

### (2)样本的可分性

 &emsp;&emsp;1. 当各类模式特征之间<big>**有明显的可分性**</big>时，可用直线或曲线(面)设计分类器，有较好的效果。
&emsp;&emsp;此分类决策为<big>**确定性分类决策**</big>，当样本属于某类时，其特征向量一定会落入对应的决策区域中，当样本不属于某类时，其特征向量一定不会落入对应的决策区域中。现有待识别的样本特征落入了某决策区域中，则它一定属于对应的类。
  
 &emsp;&emsp;2. 当各类别出现<big>**混淆现象**</big>时，则分类困难。这时需要<big>**采用统计方法**</big>，对模式样本的统计特性进行观测，分析属于哪一类的<big>**概率**</big>最大，然后按照某种判据分类，如分类错误发生的概率最小，或者是分类的风险最小。  
&emsp;&emsp;此分类决策为<big>**随机性分类决策**</big>。特征空间中有多个类，当样本属于某类时，其特征向量会以一定的概率取得不同的值，现有待识别的样本特征向量取得某值，则它按不同概率有可能属于不同的类，分类决策将它按概率的大小划归到某一类别中。  
  
![这里写图片描述](http://img.blog.csdn.net/20170916113006088?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjkzNjc2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

##  三个重要的概率和概率密度

### 先验概率 $P(\omega_i)$
&emsp;&emsp;由样本的先验知识得到先验概率，可由训练集样本估算出来。
<big>例如</big>，三类一共10个训练样本，属于w1的有2个，属于w2的有3个，属于w4的有5个，则先验概率：
$$P(\omega_1)=0.2 ，P(\omega_2)=0.3，P(\omega_3)=0.5$$

### 类条件概率密度函数 $p(x\,|\,\omega_i)$
&emsp;&emsp;类条件概率密度函数用来描述每一类中特征向量的分布情况。是样本 $x$ 在 $\omega_i$ 类条件下，出现的概率密度分布函数，也称 $p(x\,|\,\omega_i)$ 为 $\omega_i$ 关于 $x$ 的似然函数。

![这里写图片描述](http://img.blog.csdn.net/20170916110535297?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjkzNjc2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

### 后验概率 $p(\omega_i\,|\,x)$
&emsp;&emsp;后验概率为某个样本 $x$ , 属于 $\omega_i$ 类的概率，$i=1,……，c$ $\omega_i$ 是离散变量
&emsp;&emsp; 如果用先验概率 $P(\omega_i)$ 来确定待分样本的类别，依据是非常不充分的，需<big>**用类条件密度 $p(x\,|\,\omega_i)$来修正**</big>。

# 贝叶斯决策理论
&emsp;&emsp; 贝叶斯决策理论是用<big>**概率统计方法**</big>研究决策问题。其基本思想是: 已知类条件概率密度和先验概率，然后利用贝叶斯公式转换成后验概率，根据后验概率大小进行决策分类。

## 贝叶斯分类

### 贝叶斯公式
**概率推理**
&emsp;&emsp;**如有条件B，则可能会出现结果A；现出现结果A，则条件B有存在的可能。**  
&emsp;&emsp; 设试验E的样本空间为$S$, $A$为$E$的事件，$B_1,B_2,……，B_c$为$S$的一个划分，且$P(A)>0$,$\,P(B_i)>0$,则  
$$P(B_i\, |\, A) = \frac{P(A\, |\, B_i)P(B_i) }{\sum_{j=1}^{c}P(A\, |\, B_j)P(B_j)}=\frac{P(A\, |\, B_i)P(B_i) }{P(A)}$$  
<big>**其中**</big>:  
&emsp;&emsp; $P(B_i\, |\, A)$为后验概率，表示事件A(结果A)出现后，各不相容的条件$B_i$ 存在的概率，它是在结果出现后才计算得到的，因此称为“后验”。  
&emsp;&emsp;  $P(A\, |\, B_j)$为类条件概率，表示在各条件$B_i$存在时，结果事件A发生的概率。  
&emsp;&emsp;  $P(B_j)$ 称为先验概率，表示各不相容的条件$B_i$ 出现的概率，它与结果A是否出现无关，仅表示根据先验知识或主观推断。  
&emsp;&emsp;$P(A)$表达了结果A在各种条件下的总体概率。
这里A对应特征向量 **x**,$B_i$对应$\omega_i$

### 贝叶斯决策理论的已知条件
 &emsp;&emsp;1.已知决策分类的类别数为 $c$ ，各类别的状态为：
$$\omega_i\,,\,i=1,……，c$$  
&emsp;&emsp;2.已知各类别总体的概率分布(各个类别的先验概率和类条件概率密度函数)
$$P(\omega_i)\, , p(x\,|\, \omega_i)\, ,i = 1,……,c$$

### 贝叶斯决策理论欲解决的问题
 &emsp;&emsp; 如果在特征空间中观察到某一个(随机)向量，
 $$x = (x_1,x_2,……, x_d)^T$$
 那么，应该将 $x$ 分到哪一个类才是最合理的？

--- 
## 各种贝叶斯分类器

### 最小错误率贝叶斯分类器
&emsp;&emsp;当已知类别出现的先验概率 $P(\omega_i)$ 和每个类中的样本分布的类条件概率密度$P(x\,|\, \omega_i)$时，可以求得一个待分类样本属于每类的后验概率 $P(\omega_i\,|\, x)$。  
<big>**决策规则**</big>:  
&emsp;&emsp;&emsp;&emsp;两类问题中，当$P(\omega_i\,|\, x)>P(\omega_j\,|\, x)$, 判决 $x\in \omega_i ;$  
&emsp;&emsp;&emsp;&emsp;多类问题中，当$P(\omega_i\,|\,x)={max}_{1\leq j\leq c}P(\omega_j\,|\, x)$时，判决 $x\in \omega_i ;$  
&emsp;&emsp;上述的分类决策规则实为“最大后验概率分类器”，它与“最小错误率分类器”的关系可以简单分析如下:  

<big>**什么是分类错误率呢?**</big>:  
&emsp;&emsp;分类错误率是指一个分类器按照其分类决策规则对样本进行分类，在结果中发生错误的概率，此处记为 $P(e\,|\,x)$，即在随机向量x值已知的情况下，发生分类错误的概率。
&emsp;&emsp; 对于随机向量$x$ 的每一个取值，都存在一个分类错误率 $P(e\,|\,x = x_i)$
对于总体而言，错误率P(e)即为每一点分类错误率对随机向量x的期望，即:  
$$P(e) = E_x(P(e\,|\,x)) =\int P(e\,|\,x)p(x)dx  $$  

对于<big>**二分类**</big>而言：  
$$P(e\,|\,x)=\begin{cases} P(\omega_1\,|\,x),  &当\,P(\omega_2\,|\,x)>P(\omega_1\,|\,x)\\
P(\omega_2\,|\,x), &当\,P(\omega_1\,|\,x)>P(\omega_2\,|\,x)
\end{cases}$$  
则  
$$P(e)=\int P(e\,|\, x)p(x)dx  = \int_{R_1} P(\omega_2\,|\, x)p(x)dx+ \int_{R_2} P(\omega_1\,|\, x)p(x)dx$$
  
其中，$R_1$为判定为$\omega_1$ 的随机向量$x$的区域，$R_2$为判定为$\omega_2$ 的随机向量$x$的区域。即为图中阴影部分的区域面积。
最小化错误率即对于每一个判定区域，取得令$P(e\,|\,x)$ 值最小的类，则阴影区域面积最小。  

![这里写图片描述](http://img.blog.csdn.net/20170916162236765?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjkzNjc2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
  
对于<big>**c类的多分类**</big>而言：  
&emsp;&emsp;正确的分类结果是未知的,但是判定错误时,正确结果一定在除了判定类别的其他类别中, 所以:  
$$P(e\,|\,x)=1-P(\omega_i\,|\,x) \quad\,P(\omega_i\,|\,x)={max}_{1\leq j\leq c}P(\omega_j\,|\, x)
$$  
则  
$$P(e)=\int P(e\,|\, x)p(x)dx  = \sum_{i=1}^{c}\int_{R_i}(1-P(\omega_i\,|\,x))p(x)dx$$  
&emsp;&emsp; 对每个点都采取相同的策略，取最大后验概率，即最大后验概率分类器即为最小分类错误分类器。  
&emsp;&emsp;直接估算后验概率比较困难，通常利用贝叶斯公式，用先验概率和似然函数计算出来。  
$$P(\omega_i\,|\,x)= \frac{P(\omega_i)p(x\,|\,\omega_i)}{p(x)}$$  
&emsp;&emsp;而$p(x)$ 只与数据集的分布有关，与类别$\omega_i$无关，最大后验概率，即为最大$P(\omega_i)p(x\,|\,\omega_i)$。  
  
<big>**最大后验概率的其他等价形式**</big>：  
&emsp;&emsp;1. 当 $p(x\,|\,\omega_i)P(\omega_i)={max}_{1\leq j\leq n}p(x\,|\,\omega_j)P(\omega_j)$时，判决 $x\in \omega_i ;$  
&emsp;&emsp;2. 对于所有的类别$\omega_j(j\neq i)$ 都有，$l(x)=\frac{p(x\,|\,\omega_i)}{p(x\,|\,\omega_j)}>\frac{P(\omega_j)}{P(\omega_i)}$。  
&emsp;&emsp;&emsp;$l(x)$称为似然比，所以又叫最大似然比。  
&emsp;&emsp;&emsp;$\frac{P(\omega_j)}{P(\omega_i)}$ 称为似然比阈值。  
&emsp;&emsp;3. $h(x) = -\ln[l(x)] = -\ln(p(x\,|\,\omega_i))+\ln(p(x\,|\,\omega_j))<\ln(\frac{P(\omega_i)}{P(\omega_j)})$  

### 最小风险贝叶斯分类器  
&emsp;&emsp;对于不同类别，产生错误的风险是不一样的，比如将良性肿瘤误判为恶性肿瘤和将恶性肿瘤误判为良性肿瘤，显然后者造成的结果更严重，所以期望后者的分类错误率要低一些，即提高后者在分类错误计算中所占的比例。

<big>**定义**</big> 权重为 $\lambda_{ij}$，表示把 $\omega_i$ 错判为 $\omega_j$ 类的惩罚因子(通常 $\lambda_{ii} = 0$)。  
<big>**定义**</big> 条件期望损失为 $R(a_i\,|\,x)$, 表示在<big>**给定**</big>的 $x$ , 决策 $a_i$ , 此时的条件期望损失，即后验概率加权和(其他类错判为 $\omega_i$ 的加权损失):  
$$r(a_i\,|\,x) = E_{\omega|x}(\lambda_{ji}) = \sum_{j=1}^c\lambda_{ji}P(\omega_j\,|\,x) , i = 1,2,……，a$$  
<big>**其中**</big>，  
$a_i$表示一种决策，表示判定随即向量 $x$ 属于 $\omega_i$ 类, 一共有 $a = c$ 种决策(可以拒绝则为 $a = c+1$ 种)。由于 $x$ 是随机向量的观察值，不同的 $x$ 采取不同决策 $a_i$ ,其条件风险的大小是不同的。  
&emsp;&emsp; 决策a可看成随机向量 $x$ 的函数，记为 $a(x)$ ,它本身也是一个随机变量。
  
<big>**定义**</big> 期望风险为  
$$R =E_x(r(a(x)\,|\,x)) = \int r(a(x)\,|\,x)p(x)dx$$   
&emsp;&emsp;期望风险R反应对整个特征空间上所有的x的取值都采用相同的决策 $ a(x)$ 所带来的<big>平均</big>风险，而条件风险 $r(a_i\,|\,x)$ 只反映观察到某一 $x$ 的条件下采取决策 $a_i$ 所带来的风险。  
&emsp;&emsp; 如果采取每个决策行动 $a_i$ 使<big>**条件风险 $R(a_i\,|\,x)$ 最小**</big>，则对所有的 $x$ 作出决策时，其 <big>**期望风险R**</big>也必然<big>**最小**</big>。

<big>**决策规则**</big>:  
&emsp;&emsp;多类问题中，当
$$ R( a_i\,|\,x)={min}_{1\leq j\leq c}R(a_j\,|\, x)$$ 时，判决 $x\in \omega_i ;$  
   
<big>**决策步骤**</big>:  
&emsp;&emsp;1. 已知 $P(\omega_j),p(x|\omega_j),\lambda_{ij}, i,j=1,2,……,c$(不考虑拒绝策略 a =  c)   
&emsp;&emsp;2. 计算后验概率 $P(\omega_j\,|\,x),j=1,2,……,c$ （根据贝叶斯公式）  
&emsp;&emsp;3. 计算 $r(a_i\,|\,x) =\sum_{j=1}^c\lambda_{ji}P(\omega_j\,|\,x) $ （计算条件风险）
&emsp;&emsp;4. $R(a_i\,|\,x) ={min}_{1\leq j\leq c}R(a_j\,|\, x) $，则 $a = a_i$ （决策）
  
## 正态分布的贝叶斯分类器  

### 正态分布  
&emsp;&emsp; 正态分布是自然界中最常见的概率分布形式，其定义为: $$p(x) = \frac{1}{\sqrt{2\pi} \sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}} , x \in R$$  
&emsp;&emsp; 则称X服从参数为 $\sigma,\mu$ 的正态分布或高斯分布，记为 $N(\mu,\sigma^2)$  
&emsp;&emsp; 其分布函数为 $$F(x) = \frac{1}{\sqrt{2\pi} \sigma}\int_{ - \infty}^xe^{-\frac{(t-\mu)^2}{2\sigma^2}}dt$$  
&emsp;&emsp; 其中:  
&emsp;&emsp; &emsp;&emsp;$\mu = E(x) = \int_{- \infty}^{\infty}xp(x)dx$ （均值或数学期望）  
&emsp;&emsp; &emsp;&emsp;$\sigma^2 =E[(x-\mu)^2] =\int_{- \infty}^{\infty}(x-\mu)^2p(x)dx $ （方差）
  
### 多维正态分布条件下的贝叶斯分类  
&emsp;&emsp; 对于 $d$ 维正态分布，其概率密度公式为:  $$p(x) = \frac{1}{(2\pi)^{\frac{d}{2}}{|\Sigma|}^{\frac{1}{2}}}exp[{-\frac{1}{2}( \textbf{x}- \mu)^T\Sigma^{-1}( \textbf{x}-\mu)}] , \textbf{x} \in R$$  
&emsp;&emsp; 其中：  
&emsp;&emsp; &emsp;&emsp;$x = (x_1,x_2,…，x_d)^T$，$d$ 维特征向量  
&emsp;&emsp; &emsp;&emsp;$$ \mu = ({\mu}_1,{\mu}_2,…，{\mu}_d)^T $$,  $d$ 维均值向量  
&emsp;&emsp; &emsp;&emsp;$$\Sigma $$ 为 $$d\times d$$ 维协方差矩阵，$$\Sigma^{-1}$为$\Sigma$$ 的逆矩阵，$$|\Sigma|$$为$$\Sigma$$ 的行列式  
&emsp;&emsp; &emsp;&emsp;$$\Sigma = [\sigma_{ij}]_{d\times d}$$ （$$i\neq j, \sigma_{ij} $$为协方差，否则为方差）
  
![这里写图片描述](http://img.blog.csdn.net/20170916215259072?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjkzNjc2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)  
对于最小错误率贝叶斯分类器，它把样本划分到后验概率最大的那一类中，因此可以定义每一类的判别函数为:  
$$ g_i(x) = P(\omega_i\,|\,x) =  P(x\,|\,\omega_i)P(\omega_i), i=1,2,…,c $$  
  
<big>**假设**</big> 样本空间被划分到$c$ 个类别决策区域，则<big>**分类判决规则**</big>为:  
&emsp;&emsp;对于 $$g_i(x)>g_j(x),i=1,2,…,c, j\neq i$$，则$x\in \omega_i$  
此时任两个类别之间的决策边界由方程：$g_i(x)=g_j(x)$决定  
判别函数中，先验概率 $P(\omega_i)$ 是一个与特征向量无关的常量，类条件概率密度 $p(x\,|\,\omega_i)$ 则满足一定的概率分布。  

<big>**假设**</big> <big>**$p(x\,|\,\omega_i)$ 符合 $d$ 维正态分布**</big>，则判别函数为:  $$g_i(x) = \frac{P(\omega_i)}{(2\pi)^{\frac{d}{2}}{|\Sigma_i|}^{\frac{1}{2}}}exp[{-\frac{1}{2}( \textbf{x}- \mu_i)^T{\Sigma}_{i}^{-1}( \textbf{x}-\mu_i)}]$$  
该判别函数含有指数，不方便计算，考虑到对数函数是单调递增函数，可对原判别函数取对数后作为新的判别函数，即:  
$$g_i(x) =\ln{P(\omega_i)}+\ln[\frac{1}{(2\pi)^{\frac{d}{2}}{|\Sigma_i|}^{\frac{1}{2}}}exp[{-\frac{1}{2}( \textbf{x}- \mu_i)^T{\Sigma}_{i}^{-1}( \textbf{x}-\mu_i)}]]$$  
$$=\ln{P(\omega_i)}{-\frac{1}{2}( \textbf{x}- \mu_i)^T{\Sigma}_{i}^{-1}( \textbf{x}-\mu_i)}-\frac{d}{2}\ln2\pi - \frac{1}{2}\ln |\Sigma_i|$$  
令 $c_i = -\frac{d}{2}\ln2\pi - \frac{1}{2}\ln |\Sigma_i|$  
则  
$g_i(x) = {-\frac{1}{2}( \textbf{x}- \mu_i)^T{\Sigma}_{i}^{-1}( \textbf{x}-\mu_i)} + \ln{P(\omega_i)}+c_i$  
展开为:  
$$g_i(x) = -\frac{1}{2}x^T{\Sigma}_{i}^{-1}x+\frac{1}{2}x^T{\Sigma}_{i}^{-1}\mu_i-\frac{1}{2}\mu_{i}^T{\Sigma}_{i}^{-1}\mu_i+\frac{1}{2}\mu_{i}^T{\Sigma}_{i}^{-1}x  + \ln{P(\omega_i)}+c_i$$  

<big>**特殊情况**</big>:  

#### 若 $\Sigma_i = \Sigma_j = \Sigma$   
$x^T\Sigma_{i}^{-1}x $ 与类别无关，在决策面方程中与常量$c_i$效果相同，可删除。 
因为$\Sigma$ 是对称矩阵，则  
$$x^T{\Sigma}_{i}^{-1}\mu_i=\sum_{i=1}^d\mu_i\sum_{j=1}^d x_j \sigma_{ji}'=\sum_{i=1}^d\mu_i\sum_{j=1}^d x_j \sigma_{ij}'=\mu_i^T\Sigma^{-1}x$$  
$$g_i(x) = \frac{1}{2}\mu_i^T\Sigma^{-1}x-\frac{1}{2}\mu_{i}^T{\Sigma}_{i}^{-1}\mu_i+\frac{1}{2}\mu_{i}^T{\Sigma}_{i}^{-1}x  + \ln{P(\omega_i)}$$  
$$g_i(x) = \mu_i^T\Sigma^{-1}x-\frac{1}{2}\mu_{i}^T{\Sigma}_{i}^{-1}\mu_i+ \ln{P(\omega_i)}$$  
取$$\omega_i = \Sigma^{-1}\mu_i$,$\omega_{i0}= \ln{P(\omega_i)}-\frac{1}{2}\mu_{i}^T{\Sigma}_{i}^{-1}\mu_i$$
则 $$g_i(x) = \omega_i^Tx+\omega_{i0}$$  
$g_i(x)$是$x$的线性函数  

<big>**若 $P(\omega_i)\neq P(\omega_j)$**</big>  
&emsp;&emsp;**决策规则**:  $g_i(x) = \omega_i^Tx+\omega_{i0} = max_{1\leq j\leq c}{ \omega_i^Tx+\omega_{i0}}$,则 $x\in \omega_i$  
&emsp;&emsp;**决策面方程**:  $W^T(x-x_0)= 0$,  
其中$W = \Sigma^{-1}(\mu_i-\mu_j), x_0 = \frac{1}{2}(\mu_i+\mu_j)-\frac{\ln\frac{P(\omega_i)}{P(\omega_j)}(\mu_i-\mu_j)}{(\mu_i-\mu_j)^T\Sigma^{-1}(\mu_i-\mu_j)}$  
-向量 $x_0$ 与向量 $\mu_i-\mu_j$ 平行  
-向量 $W^T$ 与决策平面正交  
  
![这里写图片描述](http://img.blog.csdn.net/20170917003214475?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjkzNjc2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

<big>**若 $P(\omega_i)=P(\omega_j)$**</big>  
$W = \Sigma^{-1}(\mu_i-\mu_j), x_0 = \frac{1}{2}(\mu_i+\mu_j)$  
$x_0$为连接均值的线段中点   
$\mu_i-\mu_j$与决策平面正交  

#### 若$\Sigma_i = \sigma I^2$  
$g_i(x) = {-\frac{1}{2\sigma^2}( \textbf{x}- \mu_i)^T( \textbf{x}-\mu_i)} + \ln{P(\omega_i)}$  

<big>**若 $P(\omega_i) = P(\omega_j) = k$**</big>  
$g_i(x) = {-\frac{1}{2\sigma^2}( \textbf{x}- \mu_i)^T( \textbf{x}-\mu_i)} $  
&emsp;&emsp;**决策规则**:  $g_i(x)  = {min}_{1\leq j\leq c}{ {\frac{1}{2\sigma^2}( \textbf{x}- \mu_i)^T( \textbf{x}-\mu_i)} }$,则 $x\in \omega_i$  
      又称为 <big>**最小距离分类器**</big>  

<big>**若 $P(\omega_i)\neq P(\omega_j)$**</big>   
如果待分类的向量$x$同两类均值向量的欧氏距离相等，则归入先验概率大的那类。

![这里写图片描述](http://img.blog.csdn.net/20170917011319230?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjkzNjc2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

---
# 朴素贝叶斯分类器
&emsp;&emsp;为了保证概率密度函数估计的准确性，训练样本的数量 <big> $N$ </big>一定要足够大，样本数量随着特征空间维数<big> $l$ </big>的增加呈指数增长。因为<big> $x$ </big> 和<big> $\omega$ </big>的组合很多，假设 <big> $x_j$ </big>可能取值<big> $S_j$ </big>个，<big> $\omega$ </big> 可能取值<big> $K$ </big> 个，那么样本数量是<big> $K\prod_{j=1}^lS_j$ </big>。特别地，当<big> $x_j = x_jS$ </big>,那么数量为<big> $KS^l$ </big>。  
&emsp;&emsp;由于 <big> 数据量不足 </big>，不得不降低一些概率密度估计所要求的准确度。在此情况下，<big> 假设 </big>每个特征值 <big> $x_j , j =1,2,…，l$ </big>是<big> 统计独立 </big>的，可以得到：  
$$p(x\,|\,\omega_i)=p(x_1,x_2,…,x_l\,|\,\omega_i) = \prod_{j=1}^l p(x_j\,|\,\omega_i)\quad,\quad i = 1,2,…，c $$
这种假设，<big> $p(x_i\,|\,\omega_i)$ </big> 需要用<big> $S$ </big>个训练样本来估计概率密度，使得训练样本数量减少到了<big> $Sl$ </big>, 这种分类方式就是所谓的 <big> 朴素贝叶斯分类器 </big>。  
<big>**决策规则**</big>：
$$\omega_c = arg\,{max}_{\omega_i} \prod_{j=1}^l  p(x_j\,|\,\omega_i)\quad,\quad i =1,2,…,c$$

---
# 半朴素贝叶斯分类器  
&emsp;&emsp;属性条件独立性假设在现实中往往很难成立。于是，人们尝试着对属性条件独立性假设进行一定程度的放松，由此产生了一类称为<big> “半朴素贝叶斯分类器” </big>的学习方法。  
&emsp;&emsp;“独依赖估计”是半朴素贝叶斯最常用的一种策略，所谓独依赖就是假设每个属性在类别之外最多依赖于一个其他属性，  
即:
$$P(c\,|\,x) = \frac{P(c)}{p(x)}\prod_{i =1}^{d}p(x_i\,|\,c,pa_i)$$  
&emsp;&emsp; 其中 $pa_i$ 是属性 $x_i$ 所依赖的属性，称为 $x_i$ 的父属性。(通常属性依赖不构成环)    

<big>**概率估计**</big>：  
&emsp;&emsp;$$ P(c\,|\,x_i) = \frac{|D_{c,x_i}|+1}{|D|_+N_i}$$  
&emsp;&emsp;$$ P(x_j\,|\,c,x_i) =\frac{|D_{c,x_i,x_j}|+1}{|D_{c,x_i}|+N_j}$$  

其中,$N_i$为 第$i$个属性可能的取值数，$D_{c,x_i}$为类别为c且在第i个属性上取值为$x_i$的样本集合，$D_{c,x_i,x_j}$为类别为c且在第i个属性上取值为$x_i$,在第j个属性上取值为$x_j$的样本集合

# 贝叶斯网络
&emsp;&emsp;  现考虑属性间的高阶依赖来进一步提升泛化性能。也就是说将属性$pa_i$替换为包含$k$个属性的集合$pa_i$。此时引入了<big> 贝叶斯网 </big>。  
&emsp;&emsp; 贝叶斯网借助了有向无环图来刻画属性间的依赖关系，并使用条件概率表来描述属性的联合概率分布。一个贝叶斯网$B$由结构$G$和参数$\Theta$两部分构成，即 $B =<G,\Theta>$，网络结构$G$是一个有向无环图，其每个结点对应于一个属性，若两个属性有直接依赖关系，则它们由一条边连接起来；参数 $\Theta$ 定量描述这种依赖关系，假设属性$x_i$在$G$中的父节点集为 $\pi_i$,则$\Theta$包含了每个属性的条件概率表 $\theta_{x_i|\pi_i} =P_B(x_i\,|\,\pi_i)$pi_i。

<big>**联合概率分布定义为:**</big> $$P_B(x_1,x_2,…,x_d) = \prod_{i=1}^d P_B(x_i\,|\,\pi_i)$$  
<big>**结构:**</big>  

![这里写图片描述](http://img.blog.csdn.net/20170917160042884?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjkzNjc2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)  

**同父结构**:  
给定父节点 $a$ 的取值，则 $b$ 与 $c$ 条件独立。  
$P(a,b,c) = P(a)P(b\,|\,a)P(c\,|\,a)$  
$P(b,c\,|\,a) = \frac{P(a,b,c)}{P(a)} = \frac{P(a)P(b\,|\,a)P(c\,|\,a)}{P(a)}= P(b\,|\,a)P(c\,|\,a)$

**V型结构** :  
$a$依赖于$b,c$的联合分布。  
$P(a,b,c) = P(b)P(c)P(a\,|\,b,c)$  
$P(b,c) = \frac{P(a,b,c)}{P(a\,|\,b,c)}=P(b)P(c)$  

**顺序结构**: 给定 $a$ 的取值，则 $b$ 与 $c$ 条件独立。  
$P(a,b,c) = P(c)P(a\,|\,c)P(b\,|\,a)$  
$P(b,c\,|\,a) = \frac{P(a,b,c)}{P(a)} = \frac{P(c)P(a\,|\,c)P(b\,|\,a)}{P(a)}=\frac{P(a,c)P(b\,|\,a)}{P(a)} = P(c\,|\,a)P(b\,|\,a)$  

<big>**学习:**</big>  
给定训练集$D=\{x_1,x_2,…,x_m\}$,学习过程如下：  
$$min\, s(B\,|\,D) = min\,f(\theta)|B|-LL(B\,|\,D)$$  
$$LL(B\,|\,D) = \sum_{i=1}^{m}log P_B(x_i)$$  
$f(\theta)$表示每个参数$\theta$所需的字节数，|B|为贝叶斯网的参数个数。  

# 未知概率密度函数的估计##
## 最大似然估计
<big>**目标:**</big> 估计 $p(x\,|\,\omega_i)$  
<big>**样本集:**</big> $X = {x_1,x_2,…，x_N}$ 属于$\omega_i$类的样本  
假设每一类中的数据不影响其他类参数的估计，可以各类独立地解决这样的问题。  
假设不同样本之间具有统计的独立性：  
$$p(X;\theta) = p(x_1,x_2,…,x_N;\theta)= \prod_{k=1}^{N}p(x_k;\theta)$$  
利用最大似然法得到最优参数：  
$$\theta_{ML} = arg\, {max}_\theta \prod_{k=1}^{N}p(x_k;\theta)$$  
令似然函数对$\theta$的梯度为0  
$$\frac{\partial \prod_{k=1}^{N}p(x_k;\theta)}{\partial \,\theta} = 0$$  
对数化: $$L(\theta) = \ln \prod_{k=1}^{N}p(x_k;\theta)$$  
求导:  
$$\frac{\partial L(\theta)}{\partial\,\theta}=\sum_{k=1}^{N}\frac{\partial\,\ln\,p(x_k;\theta)}{\partial \,\theta} = \sum_{k=1}^{N}\frac{1}{p(x_k;\theta)}\frac{\partial \,p(x_k;\theta)}{\partial \,\theta} =0$$

## 最大后验概率估计(贝叶斯估计)  
最大似然估计中，认为 $\theta$ 是确定存在的参数，是不变的。但是$\theta$是个随机向量，并且有其先验概率。  
$$p(\theta\,|\,X) =\frac{p(\theta)p(X\,|\,\theta)}{p(X)}$$  
计算最大值点  
$$\theta_{MAP}: \frac{\partial}{\partial\,\theta}p(\theta\,|\, X) =0$$  
$p(x)$与$\theta$无关，则  
$$\frac{\partial}{\partial\,\theta}(p(\theta)p(X\,|\,\theta)) = 0$$  

## 贝叶斯推论  
## 最大熵估计  
熵是关于事件不确定因素的度量方法，是特征向量的随机性度量。如果$p(x)$是一个密度函数，相关的熵 $H$ 定义为  
$$H = -\int_{x}p(x)\ln\,p(x)\,dx$$  
最大熵估计是针对给定的约束条件使熵最大。  
<big>**栗子**</big>：  
当$x_1\leq x\leq x_2$时，随即变量 $x$是非零值，其余情况 $x = 0$.计算密度函数的最大熵估计。  
约束条件  
$$\int_{x_1}^{x_2}p(x)dx =1$$  
用拉格朗日乘数，相应的最大化  
$$H_L=-\int_{x_1}^{x_2}p(x)(\ln p(x)-\lambda)dx$$  
$$\frac{\partial H_l}{\partial p(x)}=-\int_{x_1}^{x_2}\{(\ln p(x)-\lambda)+1\}dx$$  
令 $\frac{\partial H_l}{\partial p(x)}= 0 $  
得到  
$$p(x) = exp(\lambda-1)$$  
利用$\int_{x_1}^{x_2}p(x)dx =1$  
得到$exp(\lambda-1)=\frac{1}{x_2-x_1}$  
因此$p(x)= \frac{1}{x_2-x_1}$  
没有其他约束，所有点服从均匀分布。

## 混合模型的参数估计
<big>**混合模型介绍**</big>  

假设一个 $J$ 分布符合 $p(x)$,则这个模型隐含的假设是每一个点 $x$ 都可能以概率 $P_j,j=1,2,…,J$ 属于 $J$ 模型分布。  
$$p(x) = \sum_{j=1}^{J}p(x\,|\,j)P_j$$  
其中  
$\sum_{j=1}^{J}P_j = 1\quad,\int_{x}p(x\,|\,j)dx =1$  
在最大似然估计中，已知类标签，使问题成为每一类独立的最大似然估计。没有标签信息使现在的任务成为一个典型的具有不完全数据集的任务。

<big>**EM算法**</big>  
混合模型的最大似然估计为  
$$\theta^{*}=arg\,{max}_{\theta}\ln\,\prod_{X}P(x;\theta)$$  
$$=arg\,{max}_{\theta}\sum_{X}\ln P(x;\theta)$$  
$$=arg\,{max}_{\theta}\sum_{X}\ln \sum_{Z}P(x,z;\theta)$$ 
和的求导比较复杂($\log(f_1(x)+f_2(x)+…,+f_n(x))$)  
利用 $Jensen$ 不等式  
如果 $f$ 是凸函数，$X$ 是随机变量，那么 $E[f(x)]\geq f(E[x])$  
特别地，如果f是严格凸函数，当且仅当 $X$ 是常量，上式取等号。
  
![这里写图片描述](http://img.blog.csdn.net/20170917183958077?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjkzNjc2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)  

$X$是随机变量，有0.5的概率是 $a$,0.5的概率是 $b$  

$f(x)$是凸函数  
$$\theta^{*}=arg\,{max}_{\theta}\sum_{X}\ln \sum_{Z}P(x,z;\theta)$$  
$$=arg\,{max}_{\theta}\sum_{X}\ln \sum_{Z}Q(z;\theta)\frac{P(x,z;\theta)}{Q(z;\theta)}$$ 
$$=arg\,{max}_{\theta}\sum_{X}\ln \sum_{Z}E_Q[\frac{P(x,z;\theta)}{Q(z;\theta)}]$$  
$$\geq arg\,{max}_{\theta}\sum_{X}\sum_{Z}E_Q[\ln (\frac{P(x,z;\theta)}{Q(z;\theta)})]$$  
$$= arg\,{max}_{\theta}\sum_{X}\sum_{Z}Q(z;\theta)\ln (\frac{P(x,z;\theta)}{Q(z;\theta)})$$  
当$\frac{P(x,z;\theta)}{Q(z;\theta)}=c$(c为任意常数)才取等号  
$$\sum_{Z} Q(z;\theta)=1$$  
$Q(z;\theta) = \frac{p(x,z;\theta)}{c}=\frac{p(x,z;\theta)}{c\sum_{Z} Q(z;\theta)}=\frac{p(x,z;\theta)}{\sum_{Z} p(x,z;\theta)}\frac{p(x,z;\theta)}{p(x;\theta)}=p(z|x;\theta)$  
$Q$为给定 $x$ 下，$z$的后验概率

**EM算法:**  

$$\theta^{*} = arg\,{max}_{\theta}\sum_{X}\sum_{Z}Q(z;\theta)\ln (\frac{P(x,z;\theta)}{Q(z;\theta)})$$  
$$=arg\,{max}_{\theta}\sum_{X}\sum_{Z}(Q(z;\theta)\ln P(x,z;\theta)-Q(z;\theta)\ln(Q(z;\theta)))$$  
第二项与$$\theta$$无关  
$$=arg\,{max}_{\theta}\sum_{X}\sum_{Z}(Q(z;\theta)\ln P(x,z;\theta))$$  
$$=arg\,{max}_{\theta}\sum_{X}\sum_{Z}(E_{Q}\ln P(x,z;\theta))$$  
E: 建立$$L(\theta)$$的下界  
M:最大化$$L(\theta)$$

## 非参数估计  
### Parzen 窗法  
### k近邻密度估计

---
# 参考资料  
1.https://baike.baidu.com/item/贝叶斯决策理论/9939173?fr=aladdin  
2.https://wenku.baidu.com/view/6d35ff4d767f5acfa1c7cd22.html  
3.https://wenku.baidu.com/view/dcfdf6c0b8f67c1cfad6b83a.html  
4.https://wenku.baidu.com/view/eaf03531ee06eff9aef80739.html  
5.https://wenku.baidu.com/view/1eb4d02da76e58fafab0037a.html  
6.http://blog.csdn.net/yangleo1987/article/details/53289387  
7.《模式识别》Sergios Theodoridis,Konstantinos Koutroumbas 第四版  
8.http://www.hankcs.com/ml/naive-bayesian-method.html 










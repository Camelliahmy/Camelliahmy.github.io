---
layout: post
title: 模式识别之K-L变换
categories:
 - 模式识别与机器学习
tags:
 - 模式识别
 - K-L变换
 - PCA
description: K-L变换是比较常用的特征生成和降低维数的方法。
---

卡洛南-洛伊(Karhunen-Loeve)变换(K-L变换): 
 
*  一种常用的特征提取方法
*  最小均方差意义下的最优正交变换
*  消除模式特征之间的相关性、突出差异性方面有最优的效果。
*  分为: 连续K-L变换、离散K-L变换

# K-L展开式


设一连续的随机实函数 $x(t)$ ,$T_1 \leq t \leq T_2$,则 $x(t)$可用已知的正交函数集 ${\phi_j(t),j=1,2,……}$ 的线性组合来展开，即:  
$x(t)=y_1\phi_1(t)+y_2\phi_2(t)+……+y_j\phi_j(t)+……$   
$x(t)=\sum_{j=1}^{\infty}y_j\phi_j(t) \quad, \qquad T_1 \leq t \leq T_2$  

函数集正交与向量正交概念类似，即任意两个函数做内积(积分)为0.  
在$T_1 \leq t \leq T_2$范围内，等间隔采样n个离散点，转化为离散形式。  
即$x(t)\rightarrow {x(1),x(2),……,x(n)}$  
$\phi_j(t)\rightarrow {\phi_j(1),\phi_j(2),……,\phi_j(n)}$  
写成向量形式:  
> $x = (x(1),x(2),…,x(n))^T$  
> $\phi_j = (\phi_j(1),\phi_j(2),…,\phi_j(n))^T$  
> $x = \sum_{j=1}^{n}y_j\phi_j=\Phi y$

其中:  
	$y = (y_1,y_2,…,y_n)^T$  
	$\Phi = (\phi_1,\phi_2,…,\phi_n)=\begin{bmatrix} \phi_1(1) & \phi_2(1) &\cdots &  \phi_n(1)\newline\phi_1(2) & \phi_2(2) &\cdots &  \phi_n(2)\newline \cdots & \cdots &\cdots & \cdots \newline 
\phi_1(n) & \phi_2(n) &\cdots &  \phi_n(n) \end{bmatrix}$  
每一列为正交函数集中的一个函数,小括号内的序号为采样点次序。

# 正交向量集的确定

$y=\Phi^Tx$  
目标是产生最优不相关的特征。  
则 $\quad E[y_iy_j]=0 \quad i\neq j$  
协方差是反映的变量之间的二阶统计特性，如果随机向量的不同分量之间的相关性很小，则所得的协方差矩阵几乎是一个对角矩阵。
而自相关矩阵相当于消除量纲的表示变量间相关性的一个矩阵。

$R_y = E[yy^T]=E[(\Phi^Tx)(\Phi^Tx)^T]=E[\Phi^Txx^T\Phi]$
因为 $\Phi$ 是固定值,所以    
$R_y = \Phi^T E[xx^T]\Phi = \Phi^T R_x\Phi$  
$R_x$是自相关矩阵，是对每一个样本自相关矩阵的加权平均  
即$R_x \approx \frac{1}{n}\sum_{k=1}^{n}x_kx_k^T$  

$R_x$ 是对称矩阵，其特征向量是互为正交的，令$\Phi$的每一列是$R_x$ 的特征向量，则$\Phi$是正交矩阵。  
$R_y = \Phi^T R_x\Phi = D_\lambda$  
$D_\lambda$是对角矩阵，其对角线上的元素是$R_x$的特征值$\lambda_i,i=0,1,…,N-1$。  
因为计算出的$R_y$只有对角元素是有值的，所以是最优不相关的变换。

# 特征选取
从正交变换后的 n维 向量中选取 m维 作为新的特征向量。  
$X = \sum_{j=1}^{n}y_j\phi_j$  
$\hat{X} = \sum_{j=1}^{m}y_j\phi_j$  
$X-\hat{X}=\sum_{j=m+1}^{n}y_j\phi_j$
$E[(X-\hat{X})^T(X-\hat{X})]=E[(\sum_{i=1}^{m}y_i\phi_i)^T(\sum_{j=1}^{m}y_j\phi_j)]=E[\sum_{i=m+1}^{n}\sum_{j=m+1}^{n}y_i\phi_i^Ty_j\phi_j]$  

$\quad \phi_i \phi_j=0 \quad i\neq j$  
$\quad \phi_i \phi_j=1 \quad i= j$  
$E[(X-\hat{X})^T(X-\hat{X})]=\sum_{j=m+1}^nE[y_i^2]$  
$E[(X-\hat{X})^T(X-\hat{X})]=\sum_{j=m+1}^nE[y_i^2]$  
由 $\quad x = \sum_{j=1}^{n}y_j\phi_j$  
得 $\quad y_j = \phi_j^Tx$  
$E[(X-\hat{X})^T(X-\hat{X})]$  
$=\sum_{j=m+1}^nE[(\phi_j^Tx)(\phi_j^Tx)^T]$  
$=\sum_{j=m+1}^nE[\phi_j^Txx^T\phi_j]$  
$=\sum_{j=m+1}^n\phi_j^TE[xx^T]\phi_j$  
$=\sum_{j=m+1}^n\phi_j^TR_x\phi_j$ 

因为 $R_x\phi_j=\lambda_j\phi_j$ ($\lambda_i$是$R_x$的特征值)。  
$E[(X-\hat{X})^T(X-\hat{X})]=\sum_{j=m+1}^n\phi_j^T\lambda_j\phi_j=\sum_{j=m+1}^n\lambda_j$  
因此，选取m个本征值最大的本征向量会使误差最小。

实际上,剩下的 n-m 个元素变成相应的均值，估计值才更接近它的均值。所以最好在将整体模式进行K-L变换之前，应先将其均值作为新坐标轴的原点。

# K-L展开式系数的计算步骤
K-L展开式系数可如下求得：  

1.	求随机向量x的自相关矩阵：$R = E[xx^T]$2.	求出矩阵R的特征值 $λ_j$ 和对应的特征向量 $\phi_j，j = 1,2,…,n$，得矩阵：3.	计算展开式系数：  
	$y=\Phi^T x$
	

 

	

















